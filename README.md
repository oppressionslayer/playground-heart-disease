# Heart Disease Prediction â€” Submission Notes

https://www.kaggle.com/competitions/playground-series-s6e2

Show casing WSRF here, i am not sure if i want to open source or commercial it, but i can unblack box easy. Check out the showcase. More to come soon
you can see from my linkedin i'm a serious professional and i really have made a serious breakthrough, i think the open source community will benefit the
most, but let's see what people think, the link below is just a huge showcase of WSRF!! Coming soon! Also see doctor.txt to see the unblackbox of this kaggle! 1200 
lines of code! Nice! 

# ABOUT ME: https://www.linkedin.com/in/wlars â€” 

I am an Automation Engineer. My background speaks for itself and my unblackbox skillz are quite literally legendary.

# SEE https://github.com/oppressionslayer/playground-heart-disease/blob/main/SUBMISSION_NOTES_UPDATED.md 

**Kaggle Playground Series S6E2**
**Team: Celestials**

# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
                                                                                                                                                                                                                  
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*

# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
    # Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
                                                                                                                                                                                                                  
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|--# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
                                                                                                                                                                                                                  
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*-------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*                                                                                                                                                                                                              
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*Heart Disease Prediction â€” Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs â€” run it, read it, learn from itHeart Disease Prediction â€” Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs â€” run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpo# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
                                                                                                                                                                                                                  
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*wer.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor â€” zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor â€” zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. E# Heart Disease Prediction â€” Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
                                                                                                                                                                                                                  
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*asy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 tre es, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v  UPDATE: Repo is freshly updated with the regime map, killer insights, and all the goods.
                                                                                                                                                                                                                  
  So here's a flex nobody asked for but everyone's getting anyway â€” my model has 1,296 human-readable IF-THEN rules. That's right. A doctor could literally print them out, grab a coffee, and read through every
  single decision path my model makes.

  Meanwhile, y'all are out here with XGBoost ensembles that even you can't explain. "Why did your model predict heart disease?" "Uh... gradient math said so?"

  I'll wait for someone to match that. I brought receipts. 1,296 of them. Each one readable by a human being with functioning eyes and a medical degree.

  The repo's updated too â€” go check out the regime map, the feature importance inversion (chest pain is #1 for 87% of patients but drops to #5 in the metabolic regime â€” your flat model could NEVER), and
  per-zone dominant features. It's all open source because science shouldn't hide behind a .pkl file.

  playground-heart-disease repo â€” come for the AUC, stay for the interpretability.

  Your move. Show me your rules. Oh wait... ðŸ˜„


  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with ou tputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease
```
Heart Disease Prediction â€” Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs â€” run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor â€” zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with outputs â€” run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor â€” zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions â€” honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 â€” The zone IS the prediction. The math IS the model.*


# Heart Disease Prediction â€” Submission Notes https://www.kaggle.com/competitions/playground-series-s6e2

# ABOUT ME:  https://www.linkedin.com/in/wlars   -- I am an Automation Engineer my background speaks for itself and my un blackbox skillz are quite literally legendary. 

# https://github.com/oppressionslayer/unblackboxer/  - Quite Literally we can now un blackbox even LLM's. It's quite literally 5th dimensional math. My background and skillz should help you understand that i don't joke, i'm an expert in my fields.

**Kaggle Playground Series S6E2**
**Team: Celestials**

---

## Approach

Using hyperdimensional mathematics and structured subspace decomposition, we didn't just predict heart disease â€” we mapped the entire decision topology of the dataset.

While most models treat this as a flat classification problem, our approach identified that the patient population contains **4 latent regimes** hiding in plain sight. These regimes were discovered through boundary analysis across a hyperdimensional feature manifold, not through brute-force hyperparameter search.

We don't tune. We solve.

---

## What We Found

The dataset contains a hidden partition structure defined by two diagnostic features operating as regime selectors. Once decomposed, the problem separates into four distinct prediction surfaces, each with fundamentally different decision logic.

### Regime Map

```
                        Diagnostic Marker B
                     Low (Normal)    High (Anomalous)
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   Diagnostic      â”‚               â”‚                  â”‚
   Marker A    Low â”‚  REGIME 0     â”‚  REGIME 1        â”‚
               â”€â”€  â”‚  352,546 pts  â”‚  200,294 pts     â”‚
                   â”‚  17.0% risk   â”‚  76.5% risk      â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
               Highâ”‚  REGIME 2     â”‚  REGIME 3        â”‚
                   â”‚  19,740 pts   â”‚  57,420 pts      â”‚
                   â”‚  70.2% risk   â”‚  96.5% risk      â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Regime 0 â€” Standard Profile (56% of population)

Low baseline risk. Classical symptomatic indicators dominate. The primary decision surface is governed by chest symptom presentation and cardiac output capacity.

Key decision boundaries:Heart Disease Prediction â€” Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs â€” run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor â€” zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
- Symptom type threshold at category 3 â†’ risk jumps from 5.9% to 37.1%
- Cardiac output above 150 â†’ protective effect (risk drops to 10.5%)
- Exercise-induced angina â†’ risk escalates from 12.5% to 46.8%

### Regime 1 â€” Elevated Marker B (32% of population)

High baseline risk at 76.5%. The decision surface shifts toward ST-segment geometry. Symptom presentation becomes secondary to electrical signal morphology.

Key boundaries:
- ST slope above threshold â†’ 87.7% risk
- ST depression above 0.9 â†’ 89.1% risk
- High cardiac output still offers partial protection (drops to 55.4%)

### Regime 2 â€” Elevated Marker A (3% of population)

The smallest regime. Age-dependent risk emergence. The decision surface incorporates temporal biological degradation alongside standard cardiac indicators.

Key boundaries:
- Age above 50 â†’ risk climbs from 58.2% to 74.5%
- ST depression above 1.0 â†’ 85.5% risk
- High cardiac output remains protective (drops to 43.6%)

### Regime 3 â€” Dual Elevation (9% of population)

**The critical discovery.**

Risk: **96.5%**. Nearly total saturation.

In this regime, the standard symptomatic indicators that drive prediction in every other regime **collapse as useful signals**. Chest symptom type â€” the #1 predictor across 87% of the population â€” drops to #5.

The decision surface is dominated by metabolic markers:
- Lipid levels
- Arterial pressure
- Biological age
- Cardiac output capacity

These patients present a fundamentally different risk topology. Standard models that average across all patients will never see this because the signal gets diluted across the majority population where symptomatic indicators dominate.

Our approach doesn't average. It decomposes.

---

## Performance

```
Metric                    Score
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AUC-ROC                   0.953
Accuracy                  88.7%
5-Fold Cross-Validation   88.57% Â± 0.06%
```

Compared to standard approaches on this dataset:

```
Method                     AUC-ROC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Our approach               0.9532
Gradient Boosting          0.9523
XGBoost (default)          0.9513
Neural Network (MLP)       0.9507
Logistic Regression        0.9500
Random Forest              0.9486
K-Nearest Neighbors        0.9400
```

We're not just competitive. We're the only submission that can tell you exactly why each prediction was made.

---

## The Deliverable

We generated a complete, standalone prediction system:

- **1,256 human-readable decision rules** across 4 regimes
- **2,614 lines of pure if-else logic** â€” zero ML library dependencies
- A doctor, an auditor, or a regulator can read every rule and understand every prediction
- No black boxes. No hidden weights. No "trust the neural network."

The model runs on a calculator. It runs on a phone. It runs on a 1990s PC. It requires nothing but basic conditional logic.

---

## Feature Importance Shift Across Regimes

This is the finding that standard models miss entirely:

```
Feature                  Regime 0   Regime 1   Regime 2   Regime 3
                         (Standard) (Marker B) (Marker A) (Dual)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Chest symptom type        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆ        â† collapses
Max cardiac output        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
ST depression             â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–ˆ       â–ˆâ–ˆ
Exercise angina           â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆ
ST slope                  â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆ
Lipid levels              â–ˆâ–ˆ         â–ˆ          â–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      â† emerges
Biological age            â–ˆâ–ˆ         â–ˆâ–ˆ         â–ˆâ–ˆâ–ˆ        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â† emerges
Arterial pressure         â–ˆâ–ˆ         â–ˆâ–ˆ         â–ˆâ–ˆ         â–ˆâ–ˆâ–ˆâ–ˆ        â† emerges
```

In Regime 3, the entire importance hierarchy inverts. Metabolic factors replace symptomatic factors as the primary predictors. This isn't noise â€” it's a real structural feature of the data that flat models wash out.

---

## Methodology

Hyperdimensional subspace decomposition with structured boundary analysis.

The approach identifies latent regime boundaries through power-of-2 aligned manifold scanning, then trains regime-specific expert ensembles that specialize in each subpopulation's unique decision topology.

We don't ask "what's the best model for all patients?" We ask "what's the best model for THIS type of patient?" and then we build four answers instead of one.

---

## Implications

If this were real patient data (it's synthetic, per Kaggle's description), the finding would be clinically significant:

**Patients with dual-elevated diagnostic markers are being risk-assessed using the wrong features.** Standard risk models weight chest symptoms heavily because that's what works for 87% of the population. But for the 9% in Regime 3, the risk is metabolic, not symptomatic. These patients could be walking around with 96.5% heart disease probability while presenting with "normal" chest symptom profiles.

The model doesn't just predict. It identifies who is being missed and why.

---

## Submission

- `submission_wsrf.csv` â€” Probability predictions for all 270,000 test patients
- This document â€” Because a number without an explanation is just a guess

---

*Celestials Research Division*
*Hyperdimensional Mathematics & Structured Subspace Analysis*
*"We don't predict. We decompose."*
