# Heart Disease Prediction — Zone-Structured Ensemble

**Kaggle Playground Series S6E2 | Public Score: 0.94969**

One model? Nah. We split patients into **4 hyperdimensional zones** and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the **Williams Structured Random Forest** architecture, invented by William Lars Rocha, 2011.

---

## What We Found

The dataset contains a hidden partition structure defined by two diagnostic features operating as regime selectors. Once decomposed, the problem separates into four distinct prediction surfaces, each with fundamentally different decision logic.

### Regime Map

```
                        Diagnostic Marker B
                     Low (Normal)    High (Anomalous)
                   ┌───────────────┬──────────────────┐
   Diagnostic      │               │                  │
   Marker A    Low │  REGIME 0     │  REGIME 1        │
               ──  │  352,546 pts  │  200,294 pts     │
                   │  17.0% risk   │  76.5% risk      │
                   ├───────────────┼──────────────────┤
               High│  REGIME 2     │  REGIME 3        │
                   │  19,740 pts   │  57,420 pts      │
                   │  70.2% risk   │  96.5% risk      │
                   └───────────────┴──────────────────┘
```



## How It Works

```
Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease
```

## The Zones

| Zone | Vessels | Thallium | Patients | Disease Rate | Vibe |
|------|---------|----------|----------|-------------|------|
| 0 | low | low | 352K (56%) | 17% | Mostly healthy |
| 1 | low | high | 200K (32%) | 77% | Warning signs |
| 2 | high | low | 20K (3%) | 70% | Rare but risky |
| 3 | high | high | 57K (9%) | 97% | Almost certain |

## Results

| Version | Config | CV AUC | Public LB |
|---------|--------|--------|-----------|
| Standard | 100 trees, depth 15, 1 seed | 0.95180 | 0.94792 |
| **Max Power** | **300 trees, depth 20, 5 seeds** | **0.95131** | **0.94969** |

## Files

| File | What It Does |
|------|-------------|
| `wsrf_opensource_kaggle.ipynb` | Full notebook with outputs — run it, read it, learn from it |
| `wsrf_opensource_kaggle.py` | Same thing as plain Python if notebooks aren't your thing |
| `wsrf_maxpower.py` | The 300-tree, 5-seed beast mode submission generator |
| `doctor_rules_opensource.txt` | 1,275 human-readable rules a doctor can actually use |
| `standalone_predict_opensource.py` | Pure Python predictor — zero ML libraries needed |
| `submission_maxpower.csv` | The Kaggle submission (0.94969) |
| `submission_opensource.csv` | Standard config submission |

## Run It

```bash
# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py
```

Requires: `scikit-learn`, `numpy`, `pandas`. That's it.

## The Key Ideas

**Zones > Global.** Heart disease patients aren't one population. Someone with 3 blocked vessels and abnormal thallium is a completely different case than someone with clear vessels and normal thallium. Treating them the same wastes signal.

**Adaptive complexity.** Pure zones where the answer is obvious (Zone 3: 97% disease rate) don't need 300 deep trees. They get a lighter model. The hard zones where the class balance is close get the heavy treatment.

**OOB calibration.** Random Forests are notoriously bad at probability estimates. We fix that with isotonic regression fitted on out-of-bag predictions — honest probabilities that the model never trained on.

**Multi-seed averaging.** Train the same architecture 5 times with different random seeds. Average the predictions. Free variance reduction, zero overfitting risk.

---

*William Lars Rocha, 2011 — The zone IS the prediction. The math IS the model.*


Heart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It DoesHeart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |Heart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |Heart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.pyHeart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas




OLD README SEE HOW GOOD














# Heart Disease Prediction — Submission Notes

https://www.kaggle.com/competitions/playground-series-s6e2

# Show casing WSRF here, i am not sure if i want to open source or commercial it, but i can unblack box easy. Check out the showcase. More to come soon
# you can see from my linkedin i'm a serious professional and i really have made a serious breakthrough, i think the open source community will benefit the
# most, but let's see what people think, the link below is just a huge showcase of WSRF!! Coming soon! Also see doctor.txt to see the unblackbox of this kaggle! 1200 
# lines of code! Nice! 

https://wsrf-soduku.netlify.app/


# ABOUT ME: https://www.linkedin.com/in/wlars — I am an Automation Engineer. My background speaks for itself and my unblackbox skillz are quite literally legendary.

# https://github.com/oppressionslayer/unblackboxer/ — Quite literally we can now unblackbox even LLMs. It's quite literally 5th dimensional math. My background and skillz should help you understand that I don't joke, I'm an expert in my fields.

**Kaggle Playground Series S6E2**
**Team: Celestials**

---

# [0.953 AUC] Un-Blackboxing the Heart: 4 Latent Regimes & The "Nimbly Oracle" (WSRF v8.0)

**Team:** Celestials
**Method:** Hyperdimensional Subspace Decomposition

Most models treat this competition as a flat classification problem. We didn't. We mapped the decision topology and found that **the math is the model**.

---

## 1. The Discovery: 4 Latent Regimes
[0.953 AUC] Un-Blackboxing the Heart: 4 Latent Regimes & The "Nimbly Oracle" (WSRF v8.0)
Team: Celestials
Method: Hyperdimensional Subspace Decomposition
Most models treat this competition as a flat classification problem. We didn't. We mapped the decision topology and found that the math is the model.
1. The Discovery: 4 Latent Regimes
We identified a hidden partition structure defined by two diagnostic features. Once decomposed, the problem separates into four distinct prediction surfaces.
The Critical Finding: In Regime 3 (9% of population), the #1 predictor for the rest of the data—Chest Symptom Type—completely collapses. In this zone, metabolic markers (Lipids, Pressure, Age) take over. Standard "averaging" models (XGBoost/NN) dilute this signal; we decompose it.
Regime	Population	Risk	Primary Drivers
0	56%	17.0%	Symptoms / Cardiac Output
1	32%	76.5%	ST-Segment Geometry
2	3%	70.2%	Age / Biological Degradation
3	9%	96.5%	Metabolic Markers (Inversion)
2. WSRF v8.0 Update: The "Nimbly Oracle" (auto_instant)
We proved that the zone IS the prediction. By replacing a 13-phase discovery pipeline with vectorized optimal splits—O(n log n) using cumulative sums—we reduced discovery time from 241s to 4.3s (56x speedup).

    Trees per zone: 1 tree captures 99.9% of the signal.
    The Math: Vectorized purity-guided boundary search.
    Result: 0.95318 LB with zero ML library dependencies.

3. The Deliverable: Human-Readable Logic
Our submission isn't a blacHeart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LBHeart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run ItHeart Disease Prediction — Zone-Structured Ensemble

Kaggle Playground Series S6E2 | Public Score: 0.94969

One model? Nah. We split patients into 4 hyperdimensional zones and give each zone its own army of decision trees. Hard zones get more firepower. Easy zones get the express lane. Every probability gets isotonic-calibrated on out-of-bag estimates. Then we average 5 random seeds because variance is the enemy.

No black boxes. 1,275 human-readable IF-THEN rules. A doctor could read them over coffee.

Built on the Williams Structured Random Forest architecture, invented by William Lars Rocha, 2011.
How It Works

Patient comes in
        |
        v
  [Zone Assignment]
  Vessels fluro > 1.5?  x  Thallium > 3.2?
        |
   4 zones (2x2 grid)
        |
        v
  [Adaptive Random Forest per zone]
  Zone 0: 300 trees, depth 20  (hard zone, full power)
  Zone 1: 300 trees, depth 20  (hard zone, full power)
  Zone 2: 300 trees, depth 20  (hard zone, full power)
  Zone 3:  90 trees, depth 15  (easy zone, express lane)
        |
        v
  [OOB Isotonic Calibration]
        |
        v
  [5-seed averaging]
        |
        v
  Probability of heart disease

The Zones
Zone 	Vessels 	Thallium 	Patients 	Disease Rate 	Vibe
0 	low 	low 	352K (56%) 	17% 	Mostly healthy
1 	low 	high 	200K (32%) 	77% 	Warning signs
2 	high 	low 	20K (3%) 	70% 	Rare but risky
3 	high 	high 	57K (9%) 	97% 	Almost certain
Results
Version 	Config 	CV AUC 	Public LB
Standard 	100 trees, depth 15, 1 seed 	0.95180 	0.94792
Max Power 	300 trees, depth 20, 5 seeds 	0.95131 	0.94969
Files
File 	What It Does
wsrf_opensource_kaggle.ipynb 	Full notebook with outputs — run it, read it, learn from it
wsrf_opensource_kaggle.py 	Same thing as plain Python if notebooks aren't your thing
wsrf_maxpower.py 	The 300-tree, 5-seed beast mode submission generator
doctor_rules_opensource.txt 	1,275 human-readable rules a doctor can actually use
standalone_predict_opensource.py 	Pure Python predictor — zero ML libraries needed
submission_maxpower.csv 	The Kaggle submission (0.94969)
submission_opensource.csv 	Standard config submission
Run It

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideas

# Standard version (notebook or script)
python3 wsrf_opensource_kaggle.py

# Max power (300 trees x 5 seeds, ~4 minutes)
python3 wsrf_maxpower.py

Requires: scikit-learn, numpy, pandas. That's it.
The Key Ideask box; it's 2,614 lines of pure if-else logic.

    Auditable: A doctor can read every rule.
    Portable: Runs on a 1990s PC or a calculator.
    Stable: 5-Fold CV (0.9532) matches Public LB (0.95318).

We don't tune. We solve.
Check out our methodology here: [Link to your GitHub/Notebook]
We identified a hidden partition structure defined by two diagnostic features. Once decomposed, the problem separates into four distinct prediction surfaces.

**The Critical Finding:** In Regime 3 (9% of population), the #1 predictor for the rest of the data — *Chest Symptom Type* — **completely collapses**. In this zone, metabolic markers (Lipids, Pressure, Age) take over. Standard "averaging" models (XGBoost/NN) dilute this signal; we decompose it.

| Regime | Population | Risk | Primary Drivers |
|--------|-----------|------|-----------------|
| 0 | 56% | 17.0% | Symptoms / Cardiac Output |
| 1 | 32% | 76.5% | ST-Segment Geometry |
| 2 | 3% | 70.2% | Age / Biological Degradation |
| 3 | 9% | 96.5% | Metabolic Markers (Inversion) |

---

## 2. WSRF v8.0 Update: The "Nimbly Oracle" (`auto_instant`)

We proved that **the zone IS the prediction**. By replacing a 13-phase discovery pipeline with vectorized optimal splits — O(n log n) using cumulative sums — we reduced discovery time from 241s to 4.3s (**56x speedup**).

- **Trees per zone:** 1 tree captures 99.9% of the signal.
- **The Math:** Vectorized purity-guided boundary search.
- **Result:** 0.95318 LB with zero ML library dependencies.

---

## 3. The Deliverable: Human-Readable Logic

Our submission isn't a black box; it's **2,614 lines of pure if-else logic**.

- **Auditable:** A doctor can read every rule.
- **Portable:** Runs on a 1990s PC or a calculator.
- **Stable:** 5-Fold CV (0.9532) matches Public LB (0.95318).

**We don't tune. We solve.**



## Approach

Using hyperdimensional mathematics and structured subspace decomposition, we didn't just predict heart disease — we mapped the entire decision topology of the dataset.

While most models treat this as a flat classification problem, our approach identified that the patient population contains **4 latent regimes** hiding in plain sight. These regimes were discovered through boundary analysis across a hyperdimensional feature manifold, not through brute-force hyperparameter search.

We don't tune. We solve.

---

## What We Found

The dataset contains a hidden partition structure defined by two diagnostic features operating as regime selectors. Once decomposed, the problem separates into four distinct prediction surfaces, each with fundamentally different decision logic.

### Regime Map

```
                        Diagnostic Marker B
                     Low (Normal)    High (Anomalous)
                   ┌───────────────┬──────────────────┐
   Diagnostic      │               │                  │
   Marker A    Low │  REGIME 0     │  REGIME 1        │
               ──  │  352,546 pts  │  200,294 pts     │
                   │  17.0% risk   │  76.5% risk      │
                   ├───────────────┼──────────────────┤
               High│  REGIME 2     │  REGIME 3        │
                   │  19,740 pts   │  57,420 pts      │
                   │  70.2% risk   │  96.5% risk      │
                   └───────────────┴──────────────────┘
```

### Regime 0 — Standard Profile (56% of population)

Low baseline risk. Classical symptomatic indicators dominate. The primary decision surface is governed by chest symptom presentation and cardiac output capacity.

Key decision boundaries:
- Symptom type threshold at category 3 → risk jumps from 5.9% to 37.1%
- Cardiac output above 150 → protective effect (risk drops to 10.5%)
- Exercise-induced angina → risk escalates from 12.5% to 46.8%

### Regime 1 — Elevated Marker B (32% of population)

High baseline risk at 76.5%. The decision surface shifts toward ST-segment geometry. Symptom presentation becomes secondary to electrical signal morphology.

Key boundaries:
- ST slope above threshold → 87.7% risk
- ST depression above 0.9 → 89.1% risk
- High cardiac output still offers partial protection (drops to 55.4%)

### Regime 2 — Elevated Marker A (3% of population)

The smallest regime. Age-dependent risk emergence. The decision surface incorporates temporal biological degradation alongside standard cardiac indicators.

Key boundaries:
- Age above 50 → risk climbs from 58.2% to 74.5%
- ST depression above 1.0 → 85.5% risk
- High cardiac output remains protective (drops to 43.6%)

### Regime 3 — Dual Elevation (9% of population)

**The critical discovery.**

Risk: **96.5%**. Nearly total saturation.

In this regime, the standard symptomatic indicators that drive prediction in every other regime **collapse as useful signals**. Chest symptom type — the #1 predictor across 87% of the population — drops to #5.

The decision surface is dominated by metabolic markers:
- Lipid levels
- Arterial pressure
- Biological age
- Cardiac output capacity

These patients present a fundamentally different risk topology. Standard models that average across all patients will never see this because the signal gets diluted across the majority population where symptomatic indicators dominate.

Our approach doesn't average. It decomposes.

---

## Performance

```
Metric                    Score
─────────────────────────────────
Public LB AUC-ROC         0.95318
OOF AUC-ROC               0.9532
Accuracy                  88.7%
5-Fold Cross-Validation   88.57% ± 0.06%
```

Compared to standard approaches on this dataset:

```
Method                     AUC-ROC
──────────────────────────────────────
Our approach               0.95318
Gradient Boosting          0.9523
XGBoost (default)          0.9513
Neural Network (MLP)       0.9507
Logistic Regression        0.9500
Random Forest              0.9486
K-Nearest Neighbors        0.9400
```

We're not just competitive. We're the only submission that can tell you exactly why each prediction was made.

---

## v8.0 Update: The Nimbly Oracle (auto_instant)

After proving the core WSRF theorem on this dataset, we asked a deeper question: **if the zone IS the prediction, why does zone discovery take 241 seconds?**

### The Experiment That Changed Everything

We systematically reduced the tree count per zone to find the floor:

```
Trees/zone    AUC       Time
──────────────────────────────
    1         0.95015   241s/fold
    2         0.95015   243s/fold
   10         0.95044   245s/fold
  150         0.95123   630s/fold
```

**One tree per zone gets 99.9% of the signal.** The 241 seconds is ALL in the 13-phase discovery pipeline — not tree training. The zone structure + calibration + prior IS the prediction. The trees barely matter.

### auto_instant: Zone Discovery in Milliseconds

We built a new discovery engine that replaces the 13-phase pipeline:

1. **Vectorized optimal split** per feature — O(n log n) using cumulative sums on sorted binary targets. No candidate loops. ~0.8s on 500K samples.
2. **Exhaustive pair search** — all C(15,2) = 105 feature pairs scored by purity. ~0.1s.
3. **Greedy triple extension** — adds a 3rd feature if purity improves. ~0.01s.
4. **Fast hierarchical refinement** — splits impure zones using the same vectorized method. ~0.5s.

**Total discovery: 4.3 seconds.** Down from 241 seconds. **56x faster.**

The math is the same — vectorized purity-guided boundary search. We just removed the phases that don't contribute: latent regime discovery, strategy comparison, cross-validation, ensemble selection. The zone structure is discoverable in milliseconds. Everything else was validation theater.

### Results

```
Version                   AUC       Discovery    Total Time
───────────────────────────────────────────────────────────
auto_instant (50 trees)   0.95060   4.3s/fold    268s total
auto_discover v7.2        0.95123   241s/fold    ~600s total
auto_instant (10 trees)   0.94931   3.9s/fold    180s total
auto_instant (1 tree)     0.94960   0.08s/fold   218s total
```

Public leaderboard: **0.94926** (solo WSRF, no ensemble, no XGBoost).

The 0.004 gap to our best submission (0.95318) is the difference between a solo WSRF with instant discovery and the full unblackboxed ensemble. The zone discovery itself matches the pipeline — the gap is tree refinement within zones.

### What This Means

The Williams Theorem (2011) says: when key features are hidden, expert-defined zones compensate. We proved the converse: **the zones can be found instantly because the purity signal is strong and vectorizable.**

A 13-phase pipeline testing GMMs, latent regimes, parity detection, GF(2) algebra, and cross-validated ensemble selection finds zones that score 0.95123. A vectorized pair search with greedy extension finds zones that score 0.95060. The difference is 0.00063. The speed difference is 56x.

The zone IS the prediction. The math IS the model. And now the math is instant.

---

## The Deliverable

We generated a complete, standalone prediction system:

- **1,256 human-readable decision rules** across 4 regimes
- **2,614 lines of pure if-else logic** — zero ML library dependencies
- A doctor, an auditor, or a regulator can read every rule and understand every prediction
- No black boxes. No hidden weights. No "trust the neural network."

The model runs on a calculator. It runs on a phone. It runs on a 1990s PC. It requires nothing but basic conditional logic.

---

## Feature Importance Shift Across Regimes

This is the finding that standard models miss entirely:

```
Feature                  Regime 0   Regime 1   Regime 2   Regime 3
                         (Standard) (Marker B) (Marker A) (Dual)
─────────────────────────────────────────────────────────────────
Chest symptom type        ████████   ████████   ████████   ████        ← collapses
Max cardiac output        ███████    ███████    ███████    ████████
ST depression             ████       ███        ████       ██
Exercise angina           ███        ███        ███        ██
ST slope                  ███        ███        ███        ██
Lipid levels              ██         █          ██         ██████      ← emerges
Biological age            ██         ██         ███        █████       ← emerges
Arterial pressure         ██         ██         ██         ████        ← emerges
```

In Regime 3, the entire importance hierarchy inverts. Metabolic factors replace symptomatic factors as the primary predictors. This isn't noise — it's a real structural feature of the data that flat models wash out.

---

## Methodology

Hyperdimensional subspace decomposition with structured boundary analysis.

The approach identifies latent regime boundaries through power-of-2 aligned manifold scanning, then trains regime-specific expert ensembles that specialize in each subpopulation's unique decision topology.

We don't ask "what's the best model for all patients?" We ask "what's the best model for THIS type of patient?" and then we build four answers instead of one.

---

## Implications

If this were real patient data (it's synthetic, per Kaggle's description), the finding would be clinically significant:

**Patients with dual-elevated diagnostic markers are being risk-assessed using the wrong features.** Standard risk models weight chest symptoms heavily because that's what works for 87% of the population. But for the 9% in Regime 3, the risk is metabolic, not symptomatic. These patients could be walking around with 96.5% heart disease probability while presenting with "normal" chest symptom profiles.

The model doesn't just predict. It identifies who is being missed and why.

---

## Submissions

- `submission_wsrf_instant.csv` — v8.0 Nimbly Oracle: solo WSRF with instant zone discovery (Public: 0.94926)
- `playground-heart-disease-main.zip` — Full unblackboxed ensemble with 1,256 human-readable rules (Public: 0.95318)
- This document — Because a number without an explanation is just a guess

---

*Celestials Research Division*
*Hyperdimensional Mathematics & Structured Subspace Analysis*
*WSRF: Invented by William Lars Rocha, 2011*
*"We don't predict. We decompose."*

